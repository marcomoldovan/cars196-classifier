# @package _global_

defaults:
  - override /hydra/sweeper: optuna

optimized_metric: "val/acc_best"

hydra:
  mode: "MULTIRUN"

  sweeper:
    _target_: hydra_plugins.hydra_optuna_sweeper.optuna_sweeper.OptunaSweeper
    storage: null
    study_name: null
    n_jobs: 1
    direction: maximize
    n_trials: 10

    sampler:
      _target_: optuna.samplers.TPESampler
      seed: 1234
      n_startup_trials: 3

    params:
      # model.optimizer._target_: choice('torch.optim.Adam', 'torch.optim.SGD', 'torch.optim.RMSprop')
      # model.optimizer.betas: choice((0.8, 0.998), (0.9, 0.999), (0.92, 0.9992)) # only for Adam
      # model.scheduler.step_size: interval(5, 50) # only for StepLR
      #     model.scheduler._target_:
      # choice('torch.optim.lr_scheduler.ReduceLROnPlateau',
      # 'torch.optim.lr_scheduler.StepLR',
      # 'torch.optim.lr_scheduler.ExponentialLR')
      # model.optimizer.momentum: interval(0.5, 0.99) # only for SGD

      # model.net.dropout_value: interval(0.3, 0.6)
      # data.batch_size: choice(32, 64, 128) # 128 does not work with swintran and mobilevit because 8GB GPU is not enough, 128 & 64 not work with densenet
      # model.net.freeze_layers: choice(True, False)
      # model.scheduler.patience: int(interval(5, 10))
      # model.scheduler.factor: interval(0.1, 0.5)
      model.optimizer.weight_decay: interval(1e-5, 1e-3)
      model.optimizer.lr: interval(1e-4, 1e-2)
      model.optimizer.momentum: interval(0.5, 0.99)
